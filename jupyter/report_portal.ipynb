{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = \"/workspace\"\n",
    "#WORKSPACE = \"/root/jupyter/report_portal/workspace\"\n",
    "YAML = WORKSPACE + \"/benchmark_config.yaml\"\n",
    "BASE_DATASTORE = WORKSPACE + \"/base.datastore.json\"\n",
    "TEST_DATASTORE = WORKSPACE + \"/test.datastore.json\"\n",
    "BASE_METADATA = WORKSPACE + \"/base.testrun_metadata.json\"\n",
    "TEST_METADATA = WORKSPACE + \"/test.testrun_metadata.json\"\n",
    "\n",
    "BASE_TESTRUN_RESULT = WORKSPACE + \"/base.testrun_result.csv\"\n",
    "TEST_TESTRUN_RESULT = WORKSPACE + \"/test.testrun_result.csv\"\n",
    "METADATA = WORKSPACE + \"/2way_metadata.csv\"\n",
    "BENCHMARK = WORKSPACE + \"/2way_benchmark.csv\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from datetime import datetime\n",
    "\n",
    "BASEPATH = os.path.abspath('.')\n",
    "SCRIPTPATH = BASEPATH + \"/fetch_scripts\"\n",
    "\n",
    "# round data to two decimals\n",
    "FORMATER=\"{:.2f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printmd(string, color=None):\n",
    "    colorstr =\"<span style='color:{}'>{}</span>\".format(color, string)\n",
    "    display(Markdown(colorstr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Test Report Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "dt_string = \"Generate time: *{}*\".format(now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "printmd(dt_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(\"Fail to load {}\".format(json_file))\n",
    "            raise\n",
    "    return data\n",
    "\n",
    "base_metadata = read_json(BASE_METADATA)\n",
    "test_metadata = read_json(TEST_METADATA)\n",
    "assert base_metadata.get(\"testrun.type\") == test_metadata.get(\"testrun.type\"), \"Base and Test type must be the same! Exit.\"\n",
    "\n",
    "run_type = base_metadata.get(\"testrun.type\")\n",
    "base_platform = base_metadata.get(\"testrun.platform\")\n",
    "test_platform = test_metadata.get(\"testrun.platform\")\n",
    "# Type and platform must not be None\n",
    "assert run_type is not None, \"Type is None! Exit.\"\n",
    "assert base_platform is not None, \"Base platform is None! Exit.\"\n",
    "assert test_platform is not None, \"Test platform is None! Exit.\"\n",
    "\n",
    "with open('{}/templates/{}_{}.md'.format(BASEPATH, base_platform.lower(), run_type), 'r') as f:\n",
    "    display(Markdown(f.read()))\n",
    "    \n",
    "if base_platform != test_platform:\n",
    "    with open('{}/templates/{}_{}.md'.format(BASEPATH, test_platform.lower(), run_type), 'r') as f:\n",
    "        display(Markdown('\\n'+f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {\n",
    "        display: inline-block\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Base testrun result\n",
    "!python3 $SCRIPTPATH/generate_testrun_results.py \\\n",
    "    --config $YAML \\\n",
    "    --datastore $BASE_DATASTORE \\\n",
    "    --metadata $BASE_METADATA \\\n",
    "    --output $BASE_TESTRUN_RESULT\n",
    "assert os.path.exists(BASE_TESTRUN_RESULT), \"Fail to generate {}! Exit.\".format(BASE_TESTRUN_RESULT)\n",
    "\n",
    "# Generate Test testrun result\n",
    "!python3 $SCRIPTPATH/generate_testrun_results.py \\\n",
    "    --config $YAML \\\n",
    "    --datastore $TEST_DATASTORE \\\n",
    "    --metadata $TEST_METADATA \\\n",
    "    --output $TEST_TESTRUN_RESULT\n",
    "assert os.path.exists(TEST_TESTRUN_RESULT), \"Fail to generate {}! Exit.\".format(TEST_TESTRUN_RESULT)\n",
    "\n",
    "# Generate 2way metadata\n",
    "!python3 $SCRIPTPATH/generate_2way_metadata.py \\\n",
    "    --test $TEST_METADATA \\\n",
    "    --base $BASE_METADATA \\\n",
    "    --output $METADATA\n",
    "assert os.path.exists(METADATA), \"Fail to generate {}! Exit.\".format(METADATA)\n",
    "\n",
    "#Generate 2way benchmark\n",
    "!python3 $SCRIPTPATH/generate_2way_benchmark.py \\\n",
    "    --config $YAML \\\n",
    "    --test $TEST_TESTRUN_RESULT \\\n",
    "    --base $BASE_TESTRUN_RESULT \\\n",
    "    --output $BENCHMARK\n",
    "assert os.path.exists(BENCHMARK), \"Fail to generate {}! Exit.\".format(BENCHMARK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_delta(val):\n",
    "    color_dict = {\n",
    "        \"Major Regression\": 'red',\n",
    "        \"Minor Regression\": 'black',\n",
    "        \"Major Improvement\": 'green',\n",
    "        \"Minor Improvement\": 'black',\n",
    "        \"Variance Too Large\": 'orange',\n",
    "        \"No Significance\": 'black',\n",
    "        \n",
    "    }\n",
    "    return 'color: {}'.format(color_dict.get(val, 'black'))\n",
    "\n",
    "def highlight_cols(s):\n",
    "    return 'background-color: #eeffff'\n",
    "\n",
    "def bold_font(s):\n",
    "    return 'font-weight: bold'\n",
    "\n",
    "def displayComparison(df):\n",
    "    #These are the columns which need special formatting\n",
    "    deltacols=df.columns.map(lambda x: x.endswith(\"-SPEC\"))\n",
    "    roundcols=df.columns.map(lambda x: x.endswith((\"-AVG\", \"-%SD\", \"-%DIFF\", \"-SIGN\")))\n",
    "    display(df.style.applymap(color_delta,subset=deltacols).applymap(bold_font,subset=deltacols).format(FORMATER, subset=roundcols).hide_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "* The differences between Test and Base are <b style='color:orange'>highlighted</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def highlight_diff(row, cell_format):\n",
    "    cell_format = cell_format if row['TEST'] != row['BASE'] else ''\n",
    "    format_row = ['', cell_format, cell_format]\n",
    "    return format_row\n",
    "\n",
    "def color_diff(row):\n",
    "    return highlight_diff(row, 'color: orange')\n",
    "\n",
    "def bold_diff(row):\n",
    "    return highlight_diff(row, 'font-weight: bold')\n",
    "    \n",
    "conf_df = pd.read_csv(METADATA, index_col=0)\n",
    "conf_df = conf_df[['KEY', 'TEST', 'BASE']]\n",
    "sorter = ['testrun.id'] + [x for x in conf_df['KEY'] if x != 'testrun.id']\n",
    "conf_df['KEY'] = conf_df['KEY'].astype(\"category\")\n",
    "conf_df[\"KEY\"].cat.set_categories(sorter, inplace=True)\n",
    "conf_df.sort_values(['KEY'], inplace=True)\n",
    "display(conf_df.style.applymap(bold_font, subset=['KEY']).apply(color_diff, axis=1).apply(bold_diff, axis=1).hide_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/templates/summary_introduction.html'.format(BASEPATH), 'r') as f:\n",
    "    display(HTML(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "benchmark_df = pd.read_csv(BENCHMARK, index_col=0)\n",
    "summary_df = benchmark_df[['RW','BS','IOdepth','Numjobs']+list(benchmark_df.filter(regex='-SPEC$').columns)]\n",
    "displayComparison(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details\n",
    "\n",
    "This section shows the detail data of benchmark report, base run result and test run result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detail benchmark report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "displayComparison(benchmark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert testrun result Path column value to link\n",
    "def make_clickable(path):\n",
    "    # target _blank to open new window\n",
    "    return '<a target=\"_blank\" href=\"../../{}\">link</a>'.format(path)\n",
    "\n",
    "def formatRunResult(df):\n",
    "    # Convert Path to link and round data\n",
    "    display(df.style.format({'Path': make_clickable}).format(FORMATER, subset=['IOPS', 'LAT(ms)', 'CLAT(ms)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base run result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_df = pd.read_csv(BASE_TESTRUN_RESULT, index_col=0)\n",
    "formatRunResult(base_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test run result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(TEST_TESTRUN_RESULT)\n",
    "formatRunResult(test_df)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
