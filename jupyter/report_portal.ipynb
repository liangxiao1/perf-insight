{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = \"/workspace\"\n",
    "YAML = WORKSPACE + \"/benchmark_config.yaml\"\n",
    "BASE_DATASTORE = WORKSPACE + \"/base.datastore.json\"\n",
    "TEST_DATASTORE = WORKSPACE + \"/test.datastore.json\"\n",
    "BASE_METADATA = WORKSPACE + \"/base.testrun_metadata.json\"\n",
    "TEST_METADATA = WORKSPACE + \"/test.testrun_metadata.json\"\n",
    "\n",
    "BASE_TESTRUN_RESULT = WORKSPACE + \"/base.testrun_result.csv\"\n",
    "TEST_TESTRUN_RESULT = WORKSPACE + \"/test.testrun_result.csv\"\n",
    "METADATA = WORKSPACE + \"/2way_metadata.csv\"\n",
    "BENCHMARK = WORKSPACE + \"/2way_benchmark.csv\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import IPython.core.display as di # Example: di.display_html('<h3>%s:</h3>' % str, raw=True)\n",
    "from datetime import datetime\n",
    "\n",
    "BASEPATH = os.path.abspath('.')\n",
    "SCRIPTPATH = BASEPATH + \"/../fetch_scripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line will hide code by default when the notebook is exported as HTML\n",
    "di.display_html('<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>', raw=True)\n",
    "\n",
    "# This line will add a button to toggle visibility of code blocks, for use with the HTML export version\n",
    "di.display_html('''<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code</button>''', raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Test Report Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "dt_string = \"Generate time: *{}*\".format(now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "display(Markdown(dt_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(\"Fail to load {}\".format(json_file))\n",
    "            raise\n",
    "    return data\n",
    "\n",
    "base_metadata = read_json(BASE_METADATA)\n",
    "test_metadata = read_json(TEST_METADATA)\n",
    "assert base_metadata.get(\"testrun.type\") == test_metadata.get(\"testrun.type\"), \"Base and Test type must be the same! Exit.\"\n",
    "\n",
    "run_type = base_metadata.get(\"testrun.type\")\n",
    "base_platform = base_metadata.get(\"testrun.platform\")\n",
    "test_platform = test_metadata.get(\"testrun.platform\")\n",
    "base_id = base_metadata.get(\"testrun.id\")\n",
    "test_id = test_metadata.get(\"testrun.id\")\n",
    "# Type and platform must not be None\n",
    "assert run_type is not None, \"Type is None! Exit.\"\n",
    "assert base_platform is not None, \"Base platform is None! Exit.\"\n",
    "assert test_platform is not None, \"Test platform is None! Exit.\"\n",
    "\n",
    "with open('{}/templates/{}_{}.md'.format(BASEPATH, base_platform.lower(), run_type), 'r') as f:\n",
    "    display(Markdown(f.read()))\n",
    "    \n",
    "if base_platform != test_platform:\n",
    "    with open('{}/templates/{}_{}.md'.format(BASEPATH, test_platform.lower(), run_type), 'r') as f:\n",
    "        display(Markdown('\\n'+f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {\n",
    "        display: inline-block\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Base testrun result script\n",
    "base_gen_result_script = \"{}/generate_testrun_results.py --config {} --datastore {} --metadata {} --output {}\".format(\n",
    "SCRIPTPATH, YAML, BASE_DATASTORE, BASE_METADATA, BASE_TESTRUN_RESULT)\n",
    "\n",
    "# Generate Test testrun result script\n",
    "test_gen_result_script = \"{}/generate_testrun_results.py --config {} --datastore {} --metadata {} --output {}\".format(\n",
    "SCRIPTPATH, YAML, TEST_DATASTORE, TEST_METADATA, TEST_TESTRUN_RESULT)\n",
    "\n",
    "# Generate 2way metadata script\n",
    "gen_metadata_script = \"{}/generate_2way_metadata.py --test {} --base {} --output {}\".format(\n",
    "SCRIPTPATH, TEST_METADATA, BASE_METADATA, METADATA)\n",
    "\n",
    "# Generate 2way benchmark script\n",
    "gen_benchmark_script = \"{}/generate_2way_benchmark.py --config {} --test {} --base {} --output {}\".format(\n",
    "SCRIPTPATH, YAML, TEST_TESTRUN_RESULT, BASE_TESTRUN_RESULT, BENCHMARK)\n",
    "\n",
    "# Run scripts parallelly\n",
    "import multiprocessing\n",
    "all_processes = (base_gen_result_script, test_gen_result_script, gen_metadata_script)   \n",
    "\n",
    "def execute(process):                                                             \n",
    "    os.system(f'python3 {process}') \n",
    "\n",
    "process_pool = multiprocessing.Pool(processes = 3)                                                        \n",
    "process_pool.map(execute, all_processes)\n",
    "\n",
    "for result in [BASE_TESTRUN_RESULT, TEST_TESTRUN_RESULT, METADATA]:\n",
    "    assert os.path.exists(result), \"Fail to generate {}! Exit.\".format(result)\n",
    "\n",
    "# Generate 2way benchmark\n",
    "os.system('python3 {}'.format(gen_benchmark_script))\n",
    "assert os.path.exists(BENCHMARK), \"Fail to generate {}! Exit.\".format(BENCHMARK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_delta(val):\n",
    "    color_dict = {\n",
    "        \"Major Regression\": 'red',\n",
    "        \"Minor Regression\": 'black',\n",
    "        \"Major Improvement\": 'green',\n",
    "        \"Minor Improvement\": 'black',\n",
    "        \"Variance Too Large\": 'orange',\n",
    "        \"No Significance\": 'black',\n",
    "        \n",
    "    }\n",
    "    return 'color: {}'.format(color_dict.get(val, 'black'))\n",
    "\n",
    "def highlight_cols(s):\n",
    "    return 'background-color: #eeffff'\n",
    "\n",
    "def bold_font(s):\n",
    "    return 'font-weight: bold'\n",
    "\n",
    "def displayComparison(df):\n",
    "    #These are the columns which need special formatting\n",
    "    deltacols=df.columns.map(lambda x: x.endswith(\"-SPEC\"))\n",
    "#    roundcols=df.columns.map(lambda x: x.endswith((\"-AVG\", \"\", \"-%SD\", \"-%DIFF\", \"-SIGN\")))\n",
    "    display(df.style.applymap(color_delta,subset=deltacols).applymap(bold_font,subset=deltacols))\n",
    "#    display(df.style.applymap(color_delta,subset=deltacols).applymap(bold_font,subset=deltacols).format(FORMATER, subset=roundcols).hide_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "* The differences between Test and Base are <b style='color:orange'>highlighted</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def highlight_diff(row, cell_format):\n",
    "    cell_format = cell_format if row['TEST'] != row['BASE'] else ''\n",
    "    format_row = ['', cell_format, cell_format]\n",
    "    return format_row\n",
    "\n",
    "def color_diff(row):\n",
    "    return highlight_diff(row, 'color: orange')\n",
    "\n",
    "def bold_diff(row):\n",
    "    return highlight_diff(row, 'font-weight: bold')\n",
    "    \n",
    "conf_df = pd.read_csv(METADATA, index_col=0)\n",
    "conf_df.fillna('', inplace=True)\n",
    "#conf_df = conf_df[['KEY', 'TEST', 'BASE']]\n",
    "#sorter = ['testrun.id'] + [x for x in conf_df['KEY'] if x != 'testrun.id']\n",
    "#conf_df['KEY'] = conf_df['KEY'].astype(\"category\")\n",
    "#conf_df[\"KEY\"].cat.set_categories(sorter, inplace=True)\n",
    "#conf_df.sort_values(['KEY'], inplace=True)\n",
    "display(conf_df.style.applymap(bold_font, subset=['KEY']).apply(color_diff, axis=1).apply(bold_diff, axis=1).hide_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}/templates/summary_introduction.html'.format(BASEPATH), 'r') as f:\n",
    "    display(HTML(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "benchmark_df = pd.read_csv(BENCHMARK, index_col=0, dtype = str, keep_default_na=False)\n",
    "summary_df = benchmark_df[['RW','BS','IOdepth','Numjobs']+list(benchmark_df.filter(regex='-SPEC$').columns)]\n",
    "displayComparison(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details\n",
    "\n",
    "This section shows the detail data of benchmark report, base run result and test run result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detail benchmark report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "displayComparison(benchmark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert testrun result Path column value to link\n",
    "def formatRunResult(df, testrun_id):\n",
    "    # Convert Path to link and round data\n",
    "    display(df.style.format({'Path': lambda x: '<a target=\"_blank\" href=\"../../testruns/{}/{}\">raw data</a>'.format(testrun_id, x)}))\n",
    "#    display(df.style.format({'Path': make_clickable}).format(FORMATER, subset=['IOPS', 'LAT(ms)', 'CLAT(ms)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base run result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_df = pd.read_csv(BASE_TESTRUN_RESULT, index_col=0, dtype = str)\n",
    "formatRunResult(base_df, base_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test run result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(TEST_TESTRUN_RESULT, index_col=0)\n",
    "formatRunResult(test_df, test_id)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
